{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:33.170466300Z",
     "start_time": "2023-11-13T14:36:31.267538100Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get Question/Answer pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare dataset for Instruct Fine Tuning\n",
    "Data is in the form\n",
    "{\"prompt\": \"A user on Stack Overflow asked the following question <QUESTION>, which of the following answers were the accepted answer? <ANSWER 1> <ANSWER 2>, . . \", \"response\": \"Answer 3 was the accepted answer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:33.376605200Z",
     "start_time": "2023-11-13T14:36:33.172466900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              STACKOVERFLOW_QUESTION:  \\\n0   I am about to build a piece of a project that ...   \n1   I am using the Photoshop's javascript API to f...   \n2   I am starting to work on a hobby project with ...   \n3   I don't remember whether I was dreaming or not...   \n4   Django view points to a function, which can be...   \n..                                                ...   \n95  That's it. If you want to document a function ...   \n96  What is the best way to layout a large django ...   \n97  In Python, given a module X and a class Y, how...   \n98  What would be your preferred way to concatenat...   \n99  I see __all__ in __init__.py files. What does ...   \n\n                               STACKOVERFLOW_RESPONSE accepted_index  \n0   RESPONSE 0 Personally, I've played with severa...              0  \n1   RESPONSE 0 open up a terminal (Applications->U...              2  \n2   RESPONSE 0 One possibility is Hudson.  It's wr...              1  \n3   RESPONSE 0 No, you were not dreaming.  Python ...              6  \n4   RESPONSE 1 If you're simply displaying data fr...              5  \n..                                                ...            ...  \n95  RESPONSE 0 It's easy, you just add a docstring...              1  \n96  RESPONSE 0 This page does a good job of addres...              1  \n97  RESPONSE 0 Here's one way to do it:\\nimport in...              0  \n98  RESPONSE 0 my_list = ['a', 'b', 'c', 'd']\\nmy_...              0  \n99  RESPONSE 0 It's a list of public objects of th...              0  \n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STACKOVERFLOW_QUESTION:</th>\n      <th>STACKOVERFLOW_RESPONSE</th>\n      <th>accepted_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am about to build a piece of a project that ...</td>\n      <td>RESPONSE 0 Personally, I've played with severa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am using the Photoshop's javascript API to f...</td>\n      <td>RESPONSE 0 open up a terminal (Applications-&gt;U...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am starting to work on a hobby project with ...</td>\n      <td>RESPONSE 0 One possibility is Hudson.  It's wr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I don't remember whether I was dreaming or not...</td>\n      <td>RESPONSE 0 No, you were not dreaming.  Python ...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Django view points to a function, which can be...</td>\n      <td>RESPONSE 1 If you're simply displaying data fr...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>That's it. If you want to document a function ...</td>\n      <td>RESPONSE 0 It's easy, you just add a docstring...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>What is the best way to layout a large django ...</td>\n      <td>RESPONSE 0 This page does a good job of addres...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>In Python, given a module X and a class Y, how...</td>\n      <td>RESPONSE 0 Here's one way to do it:\\nimport in...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>What would be your preferred way to concatenat...</td>\n      <td>RESPONSE 0 my_list = ['a', 'b', 'c', 'd']\\nmy_...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>I see __all__ in __init__.py files. What does ...</td>\n      <td>RESPONSE 0 It's a list of public objects of th...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from ACL2024.modules.util.get_root_dir import get_project_root\n",
    "\n",
    "df = pd.read_csv(os.path.join(get_project_root(), \"modules\", \"dataset\", \"test.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:33.438948700Z",
     "start_time": "2023-11-13T14:36:33.367525500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen_prompt(text_input: Dict):\n",
    "    return f\"\"\"\n",
    "    <human>: {text_input[\"STACKOVERFLOW_QUESTION\"]}\n",
    "    <assistant>: {text_input[\"STACKOVERFLOW_RESPONSE\"]}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def gen_and_tok_prompt(text_input):\n",
    "    full_input = gen_prompt(text_input)\n",
    "    tok_full_prompt = tokenizer(full_input, padding = True , truncation =True)\n",
    "    return tok_full_prompt\n",
    "\n",
    "\n",
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:55:46.384753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fe9f59d4b4245fca92e766cdd8892c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44260c25ab43416e96920de1f05d28ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-7b-instruct\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "     load_in_8bit=True,  #if you want to load the 8-bit model\n",
    "#     device_map='auto', \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:46.420979200Z",
     "start_time": "2023-11-13T14:36:46.419976500Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data = data.map(gen_and_tok_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:46.432975200Z",
     "start_time": "2023-11-13T14:36:46.422978900Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.426975300Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.430975300Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Step 5: Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:46.433976400Z",
     "start_time": "2023-11-13T14:36:46.433976400Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=25,\n",
    "    output_dir=\"output_dir\", # give the location where you want to store checkpoints\n",
    "    save_strategy='epoch',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.05,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.436976200Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Step 6: Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.438976300Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('location where you  want the model to be stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Step 7: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.441975900Z"
    }
   },
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(\"location where new model is stored\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path)\n",
    "\n",
    "model_inf = PeftModel.from_pretrained(model,\"location where new model is stored\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T14:36:46.443975700Z"
    }
   },
   "outputs": [],
   "source": [
    "# create your own prompt\n",
    "prompt = f\"\"\"\n",
    "    <human>: How can i use BDB Data Science LAB?\n",
    "    <assistant>:\n",
    "    \"\"\".strip()\n",
    "\n",
    "# encode the prompt\n",
    "encoding = tokenizer(prompt, return_tensors= \"pt\").to(model.device)\n",
    "\n",
    "# set teh generation configuration params\n",
    "gen_config = model_inf.generation_config\n",
    "gen_config.max_new_tokens = 200\n",
    "gen_config.temperature = 0.2\n",
    "gen_config.top_p = 0.7\n",
    "gen_config.num_return_sequences = 1\n",
    "gen_config.pad_token_id = tokenizer.eos_token_id\n",
    "gen_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# do the inference\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = gen_config )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
