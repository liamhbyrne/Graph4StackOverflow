{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get Question/Answer pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset for Instruct Fine Tuning\n",
    "Data is in the form\n",
    "{\"prompt\": \"A user on Stack Overflow asked the following question <QUESTION>, which of the following answers were the accepted answer? <ANSWER 1> <ANSWER 2>, . . \", \"response\": \"Answer 3 was the accepted answer\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                              STACKOVERFLOW_QUESTION:  \\\n0   I am about to build a piece of a project that ...   \n1   I am using the Photoshop's javascript API to f...   \n2   I am starting to work on a hobby project with ...   \n3   I don't remember whether I was dreaming or not...   \n4   Django view points to a function, which can be...   \n..                                                ...   \n95  That's it. If you want to document a function ...   \n96  What is the best way to layout a large django ...   \n97  In Python, given a module X and a class Y, how...   \n98  What would be your preferred way to concatenat...   \n99  I see __all__ in __init__.py files. What does ...   \n\n                               STACKOVERFLOW_RESPONSE accepted_index  \n0   RESPONSE 0 Personally, I've played with severa...              0  \n1   RESPONSE 0 open up a terminal (Applications->U...              2  \n2   RESPONSE 0 One possibility is Hudson.  It's wr...              1  \n3   RESPONSE 0 No, you were not dreaming.  Python ...              6  \n4   RESPONSE 1 If you're simply displaying data fr...              5  \n..                                                ...            ...  \n95  RESPONSE 0 It's easy, you just add a docstring...              1  \n96  RESPONSE 0 This page does a good job of addres...              1  \n97  RESPONSE 0 Here's one way to do it:\\nimport in...              0  \n98  RESPONSE 0 my_list = ['a', 'b', 'c', 'd']\\nmy_...              0  \n99  RESPONSE 0 It's a list of public objects of th...              0  \n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STACKOVERFLOW_QUESTION:</th>\n      <th>STACKOVERFLOW_RESPONSE</th>\n      <th>accepted_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am about to build a piece of a project that ...</td>\n      <td>RESPONSE 0 Personally, I've played with severa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am using the Photoshop's javascript API to f...</td>\n      <td>RESPONSE 0 open up a terminal (Applications-&gt;U...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am starting to work on a hobby project with ...</td>\n      <td>RESPONSE 0 One possibility is Hudson.  It's wr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I don't remember whether I was dreaming or not...</td>\n      <td>RESPONSE 0 No, you were not dreaming.  Python ...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Django view points to a function, which can be...</td>\n      <td>RESPONSE 1 If you're simply displaying data fr...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>That's it. If you want to document a function ...</td>\n      <td>RESPONSE 0 It's easy, you just add a docstring...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>What is the best way to layout a large django ...</td>\n      <td>RESPONSE 0 This page does a good job of addres...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>In Python, given a module X and a class Y, how...</td>\n      <td>RESPONSE 0 Here's one way to do it:\\nimport in...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>What would be your preferred way to concatenat...</td>\n      <td>RESPONSE 0 my_list = ['a', 'b', 'c', 'd']\\nmy_...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>I see __all__ in __init__.py files. What does ...</td>\n      <td>RESPONSE 0 It's a list of public objects of th...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from ACL2024.modules.util.get_root_dir import get_project_root\n",
    "\n",
    "df = pd.read_csv(os.path.join(get_project_root(), \"modules\", \"dataset\", \"test.csv\"))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def gen_prompt(text_input: Dict):\n",
    "    return f\"\"\"\n",
    "    <human>: {text_input[\"STACKOVERFLOW_QUESTION\"]}\n",
    "    <assistant>: {text_input[\"STACKOVERFLOW_RESPONSE\"]}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def gen_and_tok_prompt(text_input):\n",
    "    full_input = gen_prompt(text_input)\n",
    "    tok_full_prompt = tokenizer(full_input, padding = True , truncation =True)\n",
    "    return tok_full_prompt\n",
    "\n",
    "\n",
    "data = Dataset.from_pandas(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# model_name = \"tiiuae/falcon-7b-instruct\"\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtiiuae/falcon-7b-instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_8bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#if you want to load the 8-bit model\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;43;03m#     device_map='auto',\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtiiuae/falcon-7b-instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     12\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\graph4stackoverflow\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:479\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    475\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m get_class_from_dynamic_module(\n\u001B[0;32m    476\u001B[0m         class_ref, pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    477\u001B[0m     )\n\u001B[0;32m    478\u001B[0m     _ \u001B[38;5;241m=\u001B[39m hub_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    483\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[1;32m~\\Documents\\graph4stackoverflow\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2277\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2275\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()}\n\u001B[0;32m   2276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo GPU found. A GPU is needed for quantization.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2278\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2279\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe device_map was not initialized.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2280\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting device_map to \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:torch.cuda.current_device()}.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2281\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you want to use the model for inference, please set device_map =\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2282\u001B[0m )\n\u001B[0;32m   2283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    load_in_8bit=True,  #if you want to load the 8-bit model\n",
    "#     device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data = data.map(gen_and_tok_prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: Fine-tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=25,\n",
    "    output_dir=\"output_dir\", # give the location where you want to store checkpoints\n",
    "    save_strategy='epoch',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.05,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6: Save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained('location where you  want the model to be stored')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 7: Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(\"location where new model is stored\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path)\n",
    "\n",
    "model_inf = PeftModel.from_pretrained(model,\"location where new model is stored\" )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create your own prompt\n",
    "prompt = f\"\"\"\n",
    "    <human>: How can i use BDB Data Science LAB?\n",
    "    <assistant>:\n",
    "    \"\"\".strip()\n",
    "\n",
    "# encode the prompt\n",
    "encoding = tokenizer(prompt, return_tensors= \"pt\").to(model.device)\n",
    "\n",
    "# set teh generation configuration params\n",
    "gen_config = model_inf.generation_config\n",
    "gen_config.max_new_tokens = 200\n",
    "gen_config.temperature = 0.2\n",
    "gen_config.top_p = 0.7\n",
    "gen_config.num_return_sequences = 1\n",
    "gen_config.pad_token_id = tokenizer.eos_token_id\n",
    "gen_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# do the inference\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = gen_config )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True ))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
